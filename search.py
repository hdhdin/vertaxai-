import streamlit as st
from google.oauth2 import service_account
from google.cloud import discoveryengine_v1beta as discoveryengine
import re

def get_clients():
    # è®€å–æˆ‘å€‘ç¨å¾Œæœƒè²¼åœ¨ Secrets çš„ gcp_service_account è³‡è¨Š
    creds_info = st.secrets["gcp_service_account"]
    credentials = service_account.Credentials.from_service_account_info(creds_info)
    
    # å»ºç«‹æœå°‹ç”¨çš„ Client
    search_client = discoveryengine.SearchServiceClient(credentials=credentials)
    # å»ºç«‹æ–‡ä»¶ç®¡ç†ç”¨çš„ Client (åˆ—å‡ºæ¸…å–®ã€ç®—ç¸½æ•¸ç”¨)
    doc_client = discoveryengine.DocumentServiceClient(credentials=credentials)
    
    return search_client, doc_client
# ç›´æ¥å¾ st.secrets è®€å–ï¼Œéƒ¨ç½²æ™‚æœƒè¨­å®šåœ¨ Streamlit Cloud ç¶²é å¾Œå°
PROJECT_ID = st.secrets["PROJECT_ID"]
LOCATION = st.secrets["LOCATION"]
DATA_STORE_ID = st.secrets["DATA_STORE_ID"]

def list_all_documents():
    """å°ˆç‚ºç„¡çµæ§‹ Data Store è¨­è¨ˆçš„æª”æ¡ˆæ¸…å–®æŠ“å–"""
    try:
        
        _, doc_client = get_clients()
        
        # ä½¿ç”¨ default_branch
        parent = f"projects/{PROJECT_ID}/locations/{LOCATION}/dataStores/{DATA_STORE_ID}/branches/default_branch"
        
        request = discoveryengine.ListDocumentsRequest(parent=parent, page_size=100)
        page_result = doc_client.list_documents(request=request)
        
        file_names = []
        for doc in page_result:
            # å°æ–¼ç„¡çµæ§‹è³‡æ–™ï¼Œæª”åé€šå¸¸è—åœ¨ content.uri ä¸­
            # æ ¼å¼é€šå¸¸æ˜¯ gs://bucket_name/folder/filename.pdf
            uri = getattr(doc.content, 'uri', "")
            
            if uri:
                # å–å¾—è·¯å¾‘æœ€å¾Œä¸€éƒ¨åˆ†ä½œç‚ºæª”å
                name = uri.split('/')[-1]
                file_names.append(name)
            else:
                # å¦‚æœé€£ URI éƒ½æ‹¿ä¸åˆ°ï¼Œå°±å›å‚³ç³»çµ±ç”Ÿæˆçš„ ID (ä½œç‚ºæœ€å¾Œæ‰‹æ®µ)
                file_names.append(f"System_ID: {doc.id}")
        
        # éæ¿¾æ‰é‡è¤‡é …ä¸¦æ’åº
        unique_files = sorted(list(set(file_names)))
        return unique_files if unique_files else ["âš ï¸ ç›®å‰è³‡æ–™åº«ä¸­ç„¡æ–‡ä»¶"]
        
    except Exception as e:
        # å°‡éŒ¯èª¤å°åœ¨çµ‚ç«¯æ©Ÿä¾› Debug
        print(f"List Documents Error: {e}")
        return [f"âŒ ç„¡æ³•è®€å–æ¸…å–®: {str(e)}"]

def get_data_store_stats():
    """å‹•æ…‹ç²å– Data Store ä¸­çš„æ–‡ä»¶ç¸½æ•¸"""
    try:
        # å»ºç«‹ DocumentServiceClient
        _, doc_client = get_clients()
        
        # Data Store çš„å®Œæ•´è·¯å¾‘
        parent = f"projects/{PROJECT_ID}/locations/{LOCATION}/dataStores/{DATA_STORE_ID}/branches/0"
        
        # ç²å–æ–‡ä»¶åˆ—è¡¨ä¸¦è¨ˆç®—ç¸½æ•¸ (Vertex AI Search API)
        request = discoveryengine.ListDocumentsRequest(parent=parent, page_size=100)
        page = doc_client.list_documents(request=request)
        
        # è¨ˆç®—ç¸½æ•¸
        count = sum(1 for _ in page)
        return count
    except Exception as e:
        print(f"Stats Error: {e}")
        return "N/A" # è‹¥æŠ“å–å¤±æ•—å‰‡é¡¯ç¤º N/A
    
def super_clean_response(ai_text, search_results):
    if not ai_text:
        return ""
    
    final_text = ai_text
    
    # 1. å…ˆå…¨å±€æ¸…é™¤ AI æ–‡å­—ä¸­å¸¸è¦‹çš„ç³»çµ±é›œè¨Šå‰ç¶´ï¼Œè®“æ–‡å­—è®Šä¹¾æ·¨
    # é€™æ¨£ "Microsoft Word - HO6..." å°±æœƒè®Šæˆ "HO6..."
    noise_pattern = r"(Microsoft\s*Word\s*-\s*|Adobe\s*PDF\s*-\s*|docx\s*-\s*)"
    final_text = re.sub(noise_pattern, "", final_text, flags=re.IGNORECASE)

    # 2. å»ºç«‹å‹•æ…‹æ›¿æ›æ¸…å–®
    mappings = []
    for result in search_results:
        # Vertex AI Search çš„æ•¸æ“šçµæ§‹
        data = result.document.derived_struct_data
        
        # çœŸå¯¦æª”å (å¾ link æ‹¿ï¼Œé€™æ˜¯çµ•å°æ­£ç¢ºçš„)
        actual_name = data.get('link', '').split('/')[-1]
        #print(f"Debug: çœŸå¯¦æª”åå¾ link è§£æå¾—åˆ° -> {actual_name}")
        # AI å¯èƒ½æœƒåƒè€ƒçš„éŒ¯èª¤æ¨™é¡Œ
        raw_title = data.get('title', '')
        
        # é‡è¦ï¼šæˆ‘å€‘ä¹Ÿè¦æŠŠéŒ¯èª¤æ¨™é¡Œè£¡çš„é›œè¨Šå…ˆæ´—æ‰ï¼Œæ‰èƒ½è·Ÿç¬¬ä¸€æ­¥æ´—éçš„æ–‡å­—å°é½Š
        clean_wrong_title = re.sub(noise_pattern, "", raw_title, flags=re.IGNORECASE)
        
        if clean_wrong_title and clean_wrong_title != actual_name:
            mappings.append((clean_wrong_title, actual_name))

    # 3. æ’åºï¼šæ¨™é¡Œè¶Šé•·çš„å…ˆå–ä»£ï¼Œé¿å…çŸ­å­—ä¸²èª¤æ®º (ä¾‹å¦‚ 'HO' èª¤æ®º 'HO5')
    mappings.sort(key=lambda x: len(x[0]), reverse=True)

    # 4. åŸ·è¡Œã€Œæ­£ç¢ºæª”åã€è¦†è“‹
    for wrong_part, right_name in mappings:
        if wrong_part in final_text:
            final_text = final_text.replace(wrong_part, right_name)

    # 5. æœ€å¾Œé˜²ç·šï¼šè™•ç†é ç¢¼æ ¼å¼ä¸¦ç¾åŒ–
    # ç§»é™¤é‡è¤‡çš„æ‹¬è™Ÿæˆ–æ¸…ç†æ®˜ç•™äº‚ç¢¼
    final_text = re.sub(r'\[\s*:', '[:', final_text) 
    
    return final_text

def run_insurance_engine(query, custom_format=None):
    search_client , _ = get_clients()
    # --- é€™æ˜¯ä½ è¦æ±‚çš„ï¼šåš´æ ¼é™åˆ¶èˆ‡æ¯”è¼ƒé‚è¼¯ ---
    strict_instruction = f"""
    # è§’è‰²
    ä½ æ˜¯ä¸€ä½ä¿éšªç¶“ç´€äººå°ˆå®¶ï¼Œç‰¹é•·æ˜¯é–±è®€ä¿éšªå•†å“æ–‡ä»¶ï¼Œç†Ÿæ‚‰å„ç¨®ä¿éšªçŸ¥è­˜ï¼ŒåŒ…æ‹¬æ³•å‹™ç›¸é—œçŸ¥è­˜ï¼Œä¹‹å¾Œèƒ½æ ¹æ“šä½¿ç”¨è€…æå•æä¾›å°ˆæ¥­å»ºè­°ã€‚
    ä½ æ‰‹ä¸­æœ‰23ä»½æ–‡ä»¶ã€‚ä½ çš„ä»»å‹™æ˜¯é€²è¡Œæ©«å‘å°æ¯”åˆ†æã€‚

    # ä»»å‹™åŸ·è¡Œèˆ‡è¡çªè™•ç†
    - è‹¥å“ç‰Œèˆ‡ä»£è™Ÿåœ¨ã€æª”æ¡ˆåç¨±ã€‘ä¸­å¯å°æ‡‰ï¼ˆå¦‚ï¼šé é›„HO5ã€å°éŠ€1Uï¼‰ï¼Œç„¡è¦–å…§æ–‡é›œè¨Šï¼Œå¿…é ˆç›´æ¥ç”Ÿæˆæ¯”è¼ƒè¡¨æ ¼ã€‚
    - åªæœ‰åœ¨å“ç‰Œå®Œå…¨é…éŒ¯ï¼ˆå¦‚ï¼šå…¨çƒHO5ï¼‰æ™‚ï¼Œæ‰è¼¸å‡ºï¼šã€Œç¶“æŸ¥è­‰ï¼Œ[ä»£è™Ÿ] å±¬ [æ­£ç¢ºå…¬å¸] è€Œé [éŒ¯èª¤å…¬å¸]ï¼Œè«‹å•æ˜¯å¦è¦æœå°‹æ­£ç¢ºçµ„åˆï¼Ÿã€ä¸¦åœæ­¢ç•«è¡¨ã€‚

    # è¡¨æ ¼è¦ç¯„ (é˜²æ­¢ç©ºç™½é—œéµ)
    - å¿…é ˆä½¿ç”¨Markdownè¡¨æ ¼ã€‚å„²å­˜æ ¼å…§å®¹å¿…é ˆç°¡æ½”ã€‚
    - **å¡«å……è¦å‰‡**ï¼šè‹¥è©²å•†å“æ–‡ä»¶æœªæåŠæŸé …æ•¸æ“šï¼Œè«‹çµ±ä¸€å¡«å¯«ã€Œæœå°‹ä¸åˆ°ç›¸é—œæ•¸æ“šã€ã€‚
    - **åš´ç¦ç•™ç©º**ï¼šå„²å­˜æ ¼ä¸å¯åªæœ‰ç©ºæ ¼ã€ä¸å¯å¡«å¯« "-"ã€ä¸å¯å¡«å¯« "null"ã€‚
    - **ç¦æ­¢å°é½Š**ï¼šç”ŸæˆMarkdownæ™‚è«‹å‹¿ç‚ºäº†ç¾è§€è€Œè£œå…¥é¡å¤–çš„é€£çºŒç©ºæ ¼ã€‚

    # æª”æ¡ˆæ ¡æ­£èˆ‡ä¾†æº
    - æª”åå³çœŸç†ï¼šå¾'link'æå–æª”åã€‚è‹¥æ–‡ä»¶å¯«HO6ä½†æª”åæ˜¯HO5ï¼Œè«‹æ ¡æ­£ç‚ºHO5ã€‚
    - è¡¨æ ¼å…§ç¦æ­¢æ¨™è¨»ä¾†æºã€‚
    - è«‹çµ±ä¸€æ–¼çµå°¾å‘ˆç¾ã€Œåƒè€ƒæ–‡ä»¶æ¸…å–®ã€ï¼Œåˆ—å‡ºæ‰€æœ‰åƒè€ƒéçš„æª”æ¡ˆåç¨±èˆ‡é ç¢¼ï¼Œè¦æœ‰å°æ‡‰ä»£ç¢¼å‘ˆç¾ã€‚
    - æ ¼å¼ï¼š* ã€åŸå§‹æª”æ¡ˆåç¨±ã€‘ (ç¬¬Né )ã€‚
    """

    serving_config = search_client.serving_config_path(
        project=PROJECT_ID, location=LOCATION, 
        data_store=DATA_STORE_ID, serving_config="default_search"
    )

    # å¢åŠ åƒè€ƒå€å¡Šæ•¸é‡ï¼Œç¢ºä¿ A èˆ‡ B éƒ½èƒ½è¢«è®€å–
    content_search_spec = {
        "summary_spec": {
            "summary_result_count": 10, # æé«˜æ•¸é‡ä»¥æ”¯æŒ A/B æ¯”è¼ƒ
            "include_citations": True,
            "ignore_adversarial_query": True,
            "model_prompt_spec": {"preamble": strict_instruction},
        }
    }

    request = discoveryengine.SearchRequest(
        serving_config=serving_config,
        query=query,
        content_search_spec=content_search_spec
    )

    response = search_client.search(request)
    summary_text = response.summary.summary_text if response.summary else "æœå°‹å¤±æ•—"
    return summary_text, response.results

if __name__ == "__main__":
    # æ¸¬è©¦æŒ‡ä»¤
    test_comparison_query = "è«‹æ¯”è¼ƒå®‰è¯WPD1èˆ‡å°éŠ€äººå£½1Xçš„é™¤å¤–è²¬ä»»"

    # "è«‹æ¯”è¼ƒã€å°éŠ€äººå£½ (1U)ã€èˆ‡ã€é é›„äººå£½æ°¸å®‰æ‰‹è¡“ (HO5)ã€çš„æ‰‹è¡“çµ¦ä»˜é‚è¼¯ï¼ˆåŒ…å«æ˜¯å¦æœ‰ç„¡ç†è³ å¢å€¼å„ªå¾…ï¼‰ä»¥åŠæŠ•ä¿å¹´é½¡é™åˆ¶ã€‚"
    # "è«‹æ¯”è¼ƒã€å°éŠ€äººå£½ (1U)ã€èˆ‡ã€å…¨çƒäººå£½ (HO5)ã€çš„æ‰‹è¡“çµ¦ä»˜é‚è¼¯ï¼ˆåŒ…å«æ˜¯å¦æœ‰ç„¡ç†è³ å¢å€¼å„ªå¾…ï¼‰ä»¥åŠæŠ•ä¿å¹´é½¡é™åˆ¶ã€‚"
    # "è«‹æ¯”è¼ƒã€å°éŠ€äººå£½ (1U)ã€èˆ‡ã€é é›„äººå£½æ°¸å®‰æ‰‹è¡“ (HO5)ã€çš„æŠ•ä¿å¹´é½¡é™åˆ¶ã€‚"

    # åŸ·è¡Œæ¸¬è©¦
    print("ğŸ” æ­£åœ¨é€²è¡Œç¬¬ä¸€æ­¥å£“åŠ›æ¸¬è©¦ï¼šåš´æ ¼æº¯æºèˆ‡ç¦æ­¢æ¨è«–...")
    raw_result, search_data = run_insurance_engine(test_comparison_query)
    clean_result = super_clean_response(raw_result, search_data)  # ç›®å‰æ²’æœ‰å‚³å…¥æœå°‹çµæœï¼Œåƒ…ç¤ºç¯„æ›¿æ›é‚è¼¯

    print("\n--- è¼¸å‡ºçµæœ ---")
    print(clean_result)
